---

# ğŸš€ AWS Data Engineering Mini Project â€“ Employee Salary Aggregation

## ğŸ“Œ Core Concepts

* **S3 (Raw Zone):** Source of employee data (hourly incoming CSV/JSON).
* **Trigger:** Whenever file lands, pipeline starts automatically.
* **Lambda / Glue:** Reads data â†’ aggregates salary by country.
* **S3 (Processed Zone):** Stores aggregated results (e.g., Parquet/CSV).

**Use-case:**
ğŸ‘‰ Real-world scenario where raw HR data comes hourly, and management needs **aggregated salary insights per country**.

---

## ğŸ”„ System Design / Workflow

```
[Step.1].S3.(Raw.Data).â”€â”€â–º.Event.Trigger
.........................|
.........................â–¼
[Step.2].Lambda/Glue.Job.â”€â”€â–º.Read.employee.file
.........................|
.........................â–¼
[Step.3].Aggregate.salary.by.country.(SUM)
.........................|
.........................â–¼
[Step.4].Write.back.to.S3.(Processed.Data.Bucket)

```

---

## ğŸ› ï¸ Step-by-Step Implementation

### Step 1: S3 Setup

* Create **2 buckets**:

  * `employee-data-raw` â†’ incoming hourly files.
  * `employee-data-aggregated` â†’ output salary aggregates.

### Step 2: Event Trigger

* Configure **S3 Event Notification** on `employee-data-raw`.
* Trigger â†’ AWS Lambda (or AWS Glue if dataset is big).

### Step 3: Lambda Function (Python Example)

* Reads employee file.
* Groups data by `country`.
* Aggregates `salary`.
* Writes back results to output bucket.

```python
import boto3
import pandas as pd
import io

s3 = boto3.client('s3')

def lambda_handler(event, context):
    # 1. Get file info
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # 2. Read file from S3
    response = s3.get_object(Bucket=bucket, Key=key)
    df = pd.read_csv(response['Body'])
    
    # 3. Aggregate salary by country
    agg_df = df.groupby('country')['salary'].sum().reset_index()
    
    # 4. Write back to output bucket
    csv_buffer = io.StringIO()
    agg_df.to_csv(csv_buffer, index=False)
    s3.put_object(
        Bucket="employee-data-aggregated", 
        Key=f"aggregated/{key}", 
        Body=csv_buffer.getvalue()
    )
    return {"status": "success"}
```

### Step 4: Validation

* Check **S3 aggregated bucket** â†’ aggregated CSV appears after each file upload.
* Example output:

```
country, total_salary
India, 200000
USA, 300000
UK, 150000
```

---

## âœ… Best Practices

* **Schema consistency:** Ensure all hourly files have same schema.
* **Lambda limits:** If file > 200MB, use **Glue/Spark**.
* **S3 foldering:**

  * `raw/yyyy/mm/dd/`
  * `processed/yyyy/mm/dd/`
* Use **Parquet format** for efficiency if data grows.
* Apply **IAM roles** with least privilege.

---

## ğŸ§  Mnemonics / Visual Memory Aid

Think: **â€œR-A-A-Pâ€ â†’ Raw â†’ Aggregate â†’ Append â†’ Processedâ€**

```
Raw Data (S3) â†’ Aggregate (Lambda/Glue) â†’ Append Results â†’ Processed Data (S3)
```

---

## âš ï¸ Common Pitfalls

âŒ Forgetting to configure **S3 event notifications** â†’ pipeline wonâ€™t trigger.
âŒ Large files â†’ Lambda timeouts â†’ use Glue.
âŒ Schema mismatch in CSV files â†’ Aggregation errors.
âŒ Overwriting output files instead of partitioning by time.

---

## ğŸ“ Short Revision Notes

* **S3 â†’ Lambda â†’ Salary Aggregation â†’ S3 Output**
* Use **event-driven triggers** for automation.
* Aggregate using **groupby SUM(country)**.
* Store results in a **separate processed bucket**.
* Watch out for **file size + schema mismatch**.

---
